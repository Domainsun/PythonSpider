#### urllib库的使用

Python提供了很多的库用来抓取网络资源，就从urllib库开始吧。

> urllib官方文档： https://docs.python.org/3/library/urllib.html

##### urlopen
在使用urlopen之前，我们可以在浏览器中查看百度主页的源码(鼠标右键，查看源码)

![](/Python-spider-tutorial/Images/2.1.jpg)

也就是说，浏览器收到的就是服务器给的这些东西。

那么，怎么用Python达到同样的效果呢？先来段代码：
```
# 导入urlopen
from urllib.request import urlopen
# 向指定的url发送请求，并返回服务器响应的类文件对象
response = urlopen('http://www.baidu.com')
# 类文件对象支持文件对象的操作方法，如read()方法读取文件全部内容
html = response.read().decode('utf-8')
# 打印字符串
print(html)
```
运行结果如下：

![](/Python-spider-tutorial/Images/2.2.jpg)

通过上面几行代码，我们就把百度首页的源码爬取下来了。

urlopen的返回对象还支持:

`response.getcode() # 当前爬取网页的状态码`

`response.geturl() # 当前所爬取的URL地址`

`response.info() # 对象信息`

##### Request

有些网站为了防止恶意采集信息进行了一些反爬虫设置，如果我们又要爬取这些信息，该怎么办呢？

一种常见的方法就是，修改请求报文的头部信息，让其伪装成浏览器。这个时候，我们就可以用到Request对象。

```
# 导入urlopen
from urllib.request import urlopen
# 导入Request
from urllib.request import Request
# 浏览器的User-Agent
user_agent = {"User-Agent" : "Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"}
# 构造Request对象,伪装成浏览器
request = Request('http://www.baidu.com', headers=user_agent)
# 根据Request对象发出请求
response = urlopen(request)
# 类文件对象支持文件对象的操作方法，如read()方法读取文件全部内容
html = response.read().decode('utf-8')
# 打印字符串
print(html)
```
![](/Python-spider-tutorial/Images/2.3.jpg)

可以通过调用`Request.add_header()` 添加/修改一个特定的header 也可以通过调用`Request.get_header()`来查看已有的header。

比如：

`request.add_header("Connection", "keep-alive")`

`request.get_header('User-Agent')`

##### 超时设置

有的时候网页长时间未响应，那么系统就会判断该网页超时了，即无法打开该网页。 
若要根据自己的需要来设置超时的时间值。我们可以在`urllib.request.urlopen()`打开网址的时候，通过timeout字段设置。 

设置格式为：urllib.request.urlopen(url，timeout=时间值)

##### Get请求

用百度搜索关键字时，其查询信息是通过URL传递的，比如，输入“苹果”进行搜索后，其URL变为：
`https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&tn=baidu&wd=苹果&oq=%25E8%258B%25B9%25E6%259E%259C&rsv_pq=a88bc72e00058f95&rsv_t=e100RR4O5qf00cQAILH5VoHgDntjXeOEsAPQXVOEhqxo6mfvJFCKii92mjA&rqlang=cn&rsv_enter=0`

很长的一段，但其实用`www.baidu.com/s?wd=苹果`也可以得到同样的内容。由此，我们只要按照`www.baidu.com/s?wd=关键字`的格式发送请求，就可以获取到搜索结果，这其实就是一个Get请求。

那就不妨写代码实现一下，直接上代码：
```
# 导入urlopen
from urllib.request import urlopen
# 导入Request
from urllib.request import Request
# 导入urlencode对查询字段编码
from urllib.parse import urlencode
# 浏览器的User-Agent
user_agent = {"User-Agent" : "Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"}
# 构造Request对象
base_url = 'http://www.baidu.com/s?'
query = {'wd':'苹果'}
url = base_url + urlencode(query)
request = Request(url, headers=user_agent)
# 根据Request对象发出请求
response = urlopen(request)
# 类文件对象支持文件对象的操作方法，如read()方法读取文件全部内容
html = response.read().decode('utf-8')
# 打印字符串
print(html)
```
##### Post请求

我们也会遇见要发送Post请求的情况，

![](/Python-spider-tutorial/Images/2.4.jpg)

如图，查看chrome浏览器，发现此时发送的是Post请求，发送的参数如下：

![](/Python-spider-tutorial/Images/2.5.jpg)

这个时候，我们就要构建一个Post请求，Request请求对象的里有data参数，它就是用在Post里的，我们要传送的数据就是这个参数data，data是一个字典，里面要匹配键值对。

```
import urllib.request
from urllib.request import Request
from urllib.parse import urlencode
import json

headers = {
    'Content-Type':'application/x-www-form-urlencoded; charset=UTF-8',
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36',
    'X-Requested-With':'XMLHttpRequest'
}

url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule&smartresult=ugc&sessionFrom=null'

formdata = {
    'i':'苹果',
    'from':'AUTO',
    'to':'AUTO',
    'smartresult':'dict',
    'client':'fanyideskweb',
    'doctype':'json',
    'keyfrom':'fanyi.web',
    'action':'FY_BY_REALTIME',
    'typoResult':'false',
    'salt':'1514620653413',
    'sign':'d8c85a305d0344f901318545c6bac98e',
    'version':'2.1'
}

data = urlencode(formdata)
request = Request(url, data=data.encode('utf-8'), headers=headers)
response = urllib.request.urlopen(request)
dic = json.loads(response.read().decode('utf-8'))
print(dic)
```
##### 自定义opener

基本的`urlopen()`函数不支持验证、cookie或者其他高级HTTP功能。要支持这些功能，必须使用`build_opener()`函数创建自己的自定义opener对象：
```
build_opener([handler1[,handler2,...]])
```
构建用于打开URL的自定义opener对象。参数handler1、handler2等都是特殊处理程序对象的实例。这些处理程序的目的是向得到的opener对象添加各种功能。下表列出了所有可用的处理程序对象。

处理程序|描述
----|----
CacheFTPHandler|具有持久FTP连接的FTP处理程序
FileHandler|打开本地文件
FTPHandler|通过FTP打开URL
HTTPBasicAuthHandler|基本的HTTP验证处理
HTTPCookieProcessor|处理HTTP cookie
HTTPDefaultEroorHandler|通过引发HTTPError异常来处理HTTP错误
HTTPDigestAuthHandler|HTTP摘要验证处理
HTTPHandler|通过HTTP打开URL
ProxyHandler|通过代理重定向请求
ProxyBasicAuthHandler|基本的代理验证
ProxyDigestAuthHandler|摘要代理验证
UnknownHandler|处理所有未知URL的处理程序

`build_opener()`返回的对象具有`open(url,[,data,[,timeout]])`方法，其作用是根据各种处理程序提供的规则打开URL。`open()`的参数与传递给`urlopen()`函数的参数相同。

`install_opener(opener)`可以安装不同的opener对象作为urlopen()使用的全局URL opener.

举个例子：
如果需要通过代理重定向请求，可创建ProxyHandler实例。
```
proxy = ProxyHandler({'http':'http://someproxy.com:8080'})
auth = HTTPBasicAuthHandler()
auth.add_password('realm','host','username','password')

opener = build_opener(proxy,auth)
u = opener.open('http://www.baidu.com')
```

##### 获取Ajax加载的内容
有些网页内容使用AJAX加载，一般返回的是JSON,直接对AJAX地址进行Post或Get，就返回JSON数据了。 


以上。

![](/Python-spider-tutorial/Images/0.png)