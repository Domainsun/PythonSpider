#### urllib库的使用

Python提供了很多的库用来抓取网络资源，就从urllib库开始吧。

> urllib官方文档： https://docs.python.org/3/library/urllib.html

##### urlopen
在使用urlopen之前，我们可以在浏览器中查看百度主页的源码(鼠标右键，查看源码)

![](/Python-spider-tutorial/Images/2.1.jpg)

也就是说，浏览器收到的就是服务器给的这些东西。

那么，怎么用Python达到同样的效果呢？先来段代码：
```
# 导入urlopen
from urllib.request import urlopen
# 向指定的url发送请求，并返回服务器响应的类文件对象
response = urlopen('http://www.baidu.com')
# 类文件对象支持文件对象的操作方法，如read()方法读取文件全部内容
html = response.read().decode('utf-8')
# 打印字符串
print(html)
```
运行结果如下：

![](/Python-spider-tutorial/Images/2.2.jpg)

通过上面几行代码，我们就把百度首页的源码爬取下来了。

urlopen的返回对象还支持:

`response.getcode() # 当前爬取网页的状态码`

`response.geturl() # 当前所爬取的URL地址`

`response.info() # 对象信息`

##### Request

有些网站为了防止恶意采集信息进行了一些反爬虫设置，如果我们又要爬取这些信息，该怎么办呢？

一种常见的方法就是，修改请求报文的头部信息，让其伪装成浏览器。这个时候，我们就可以用到Request对象。

```
# 导入urlopen
from urllib.request import urlopen
# 导入Request
from urllib.request import Request
# 浏览器的User-Agent
user_agent = {"User-Agent" : "Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"}
# 构造Request对象,伪装成浏览器
request = Request('http://www.baidu.com', headers=user_agent)
# 根据Request对象发出请求
response = urlopen(request)
# 类文件对象支持文件对象的操作方法，如read()方法读取文件全部内容
html = response.read().decode('utf-8')
# 打印字符串
print(html)
```
![](/Python-spider-tutorial/Images/2.3.jpg)

可以通过调用`Request.add_header()` 添加/修改一个特定的header 也可以通过调用`Request.get_header()`来查看已有的header。

比如：

`request.add_header("Connection", "keep-alive")`

`request.get_header('User-Agent')`

##### 超时设置

有的时候网页长时间未响应，那么系统就会判断该网页超时了，即无法打开该网页。 
若要根据自己的需要来设置超时的时间值。我们可以在`urllib.request.urlopen()`打开网址的时候，通过timeout字段设置。 

设置格式为：urllib.request.urlopen(url，timeout=时间值)

##### Get请求

用百度搜索关键字时，其查询信息是通过URL传递的，比如，输入“苹果”进行搜索后，其URL变为：
`https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&tn=baidu&wd=苹果&oq=%25E8%258B%25B9%25E6%259E%259C&rsv_pq=a88bc72e00058f95&rsv_t=e100RR4O5qf00cQAILH5VoHgDntjXeOEsAPQXVOEhqxo6mfvJFCKii92mjA&rqlang=cn&rsv_enter=0`

很长的一段，但其实用`www.baidu.com/s?wd=苹果`也可以得到同样的内容。由此，我们只要按照`www.baidu.com/s?wd=关键字`的格式发送请求，就可以获取到搜索结果，这其实就是一个Get请求。

那就不妨写代码实现一下，直接上代码：
```
# 导入urlopen
from urllib.request import urlopen
# 导入Request
from urllib.request import Request
# 导入urlencode对查询字段编码
from urllib.parse import urlencode
# 浏览器的User-Agent
user_agent = {"User-Agent" : "Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"}
# 构造Request对象
base_url = 'http://www.baidu.com/s?'
query = {'wd':'苹果'}
url = base_url + urlencode(query)
request = Request(url, headers=user_agent)
# 根据Request对象发出请求
response = urlopen(request)
# 类文件对象支持文件对象的操作方法，如read()方法读取文件全部内容
html = response.read().decode('utf-8')
# 打印字符串
print(html)
```
##### Post请求

我们也会遇见要发送Post请求的情况，

![](/Python-spider-tutorial/Images/2.4.jpg)

如图，查看chrome浏览器，发现此时发送的是Post请求，发送的参数如下：

![](/Python-spider-tutorial/Images/2.5.jpg)

这个时候，我们就要构建一个Post请求，Request请求对象的里有data参数，它就是用在Post里的，我们要传送的数据就是这个参数data，data是一个字典，里面要匹配键值对。

```
# 导入urlopen
from urllib.request import urlopen
# 导入Request
from urllib.request import Request
# 导入urlencode对中文字编码
from urllib.parse import urlencode

formdata = {
    'i':'python',
    'from':'AUTO',
    'to':'AUTO',
    'smartresult':'dict',
    'client':'fanyideskweb',
    'doctype':'json',
    'keyfrom':'fanyi.web',
    'action':'FY_BY_REALTIME',
    'typoResult':'false',
    'salt':'1514620653413',
    'sign':'d8c85a305d0344f901318545c6bac98e',
    'version':'2.1'
}

headers = {
    "User-Agent":"Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
}
# 构造Request对象
url = 'http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule'
formdata = urlencode(formdata).encode('utf-8')
request = Request(url, headers=headers, data=formdata)
# 根据Request对象发出请求
response = urlopen(request)
# 类文件对象支持文件对象的操作方法，如read()方法读取文件全部内容
html = response.read().decode('utf-8')
print(html)
```
##### 获取Ajax加载的内容
有些网页内容使用AJAX加载，一般返回的是JSON,直接对AJAX地址进行Post或Get，就返回JSON数据了。 


以上。

![](/Python-spider-tutorial/Images/0.png)